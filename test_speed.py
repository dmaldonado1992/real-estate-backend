#!/usr/bin/env python3
"""
Prueba de velocidad y rendimiento del modelo Llama optimizado
"""
import requests
import time
import json

def test_speed_comparison():
    """Compara la velocidad del nuevo modelo vs el anterior"""
    
    print("ğŸš€ Probando velocidad del modelo Llama 3.2:1b optimizado\n")
    
    # URL base del API
    base_url = "http://127.0.0.1:8000"
    
    # Pruebas a realizar
    tests = [
        {
            "name": "ConexiÃ³n bÃ¡sica",
            "url": f"{base_url}/api/llm/test",
            "method": "GET"
        },
        {
            "name": "DescripciÃ³n de producto",
            "url": f"{base_url}/api/llm/description",
            "method": "POST",
            "params": {"product_name": "iPhone 15 Pro Max"}
        },
        {
            "name": "Recomendaciones",
            "url": f"{base_url}/api/llm/recommendations", 
            "method": "POST",
            "params": {
                "product_description": "Smartphone de alta gama con cÃ¡mara profesional",
                "num_recommendations": 3
            }
        }
    ]
    
    results = []
    
    for test in tests:
        print(f"ğŸ§ª Ejecutando: {test['name']}")
        
        try:
            start_time = time.time()
            
            if test['method'] == 'GET':
                response = requests.get(test['url'], timeout=30)
            else:
                response = requests.post(test['url'], params=test.get('params', {}), timeout=30)
            
            end_time = time.time()
            duration = end_time - start_time
            
            if response.status_code == 200:
                result = response.json()
                print(f"   âœ… Ã‰xito en {duration:.2f}s")
                
                # Mostrar resultado relevante
                if 'description' in result:
                    print(f"   ğŸ“ DescripciÃ³n: {result['description'][:100]}...")
                elif 'recommendations' in result:
                    print(f"   ğŸ’¡ Recomendaciones: {len(result['recommendations'])} generadas")
                elif 'message' in result:
                    print(f"   ğŸ’¬ Respuesta: {result['message']}")
                
                results.append({
                    'test': test['name'],
                    'duration': duration,
                    'status': 'success',
                    'model': result.get('model', 'N/A')
                })
            else:
                print(f"   âŒ Error {response.status_code}: {response.text}")
                results.append({
                    'test': test['name'],
                    'duration': duration,
                    'status': 'error',
                    'error': response.status_code
                })
                
        except requests.Timeout:
            print(f"   â±ï¸  Timeout despuÃ©s de 30s")
            results.append({
                'test': test['name'],
                'duration': 30,
                'status': 'timeout'
            })
        except Exception as e:
            print(f"   ğŸ’¥ Error: {e}")
            results.append({
                'test': test['name'],
                'duration': 0,
                'status': 'error',
                'error': str(e)
            })
        
        print()
    
    # Resumen de resultados
    print(f"{'='*60}")
    print("ğŸ“Š RESUMEN DE RENDIMIENTO")
    print(f"{'='*60}")
    
    total_time = sum(r['duration'] for r in results if r['status'] == 'success')
    successful_tests = len([r for r in results if r['status'] == 'success'])
    
    for result in results:
        status_emoji = {
            'success': 'âœ…',
            'error': 'âŒ',
            'timeout': 'â±ï¸'
        }.get(result['status'], 'â“')
        
        print(f"{result['test']:.<35} {status_emoji} {result['duration']:.2f}s")
    
    print(f"\nğŸ“ˆ EstadÃ­sticas:")
    print(f"   â€¢ Pruebas exitosas: {successful_tests}/{len(results)}")
    print(f"   â€¢ Tiempo total: {total_time:.2f}s")
    if successful_tests > 0:
        print(f"   â€¢ Tiempo promedio: {total_time/successful_tests:.2f}s")
    
    # ComparaciÃ³n con modelo anterior
    print(f"\nğŸ† Ventajas del modelo Llama 3.2:1b:")
    print("   â€¢ ğŸš€ SÃºper rÃ¡pido - Menos parÃ¡metros = Mayor velocidad")
    print("   â€¢ ğŸ”‹ Eficiente - Menor uso de memoria y CPU")
    print("   â€¢ ğŸŒ Compatible con API de Ollama")
    print("   â€¢ ğŸ’¡ Optimizado para respuestas rÃ¡pidas")
    
def test_model_info():
    """Obtiene informaciÃ³n del modelo actual"""
    
    print("\nğŸ” InformaciÃ³n del modelo actual:")
    
    try:
        response = requests.get("http://127.0.0.1:8000/api/llm/test")
        if response.status_code == 200:
            result = response.json()
            print(f"   ğŸ“‹ Modelo: {result.get('model', 'N/A')}")
            print(f"   ğŸ”Œ Estado: {result.get('status', 'N/A')}")
            print(f"   ğŸ’¬ Mensaje: {result.get('message', 'N/A')}")
        else:
            print(f"   âŒ Error obteniendo info: {response.status_code}")
    except Exception as e:
        print(f"   ğŸ’¥ Error: {e}")

def main():
    """FunciÃ³n principal"""
    
    print("ğŸ¦™ Prueba de Rendimiento - Llama 3.2:1b Optimizado")
    print("=" * 60)
    
    # Verificar que el servidor estÃ© ejecutÃ¡ndose
    try:
        requests.get("http://127.0.0.1:8000/docs", timeout=5)
        print("âœ… Servidor detectado en http://127.0.0.1:8000")
    except:
        print("âŒ Servidor no encontrado. AsegÃºrate de que estÃ© ejecutÃ¡ndose:")
        print("   uvicorn app.main:app --host 127.0.0.1 --port 8000")
        return
    
    # Ejecutar pruebas
    test_model_info()
    test_speed_comparison()
    
    print(f"\nğŸ¯ ConfiguraciÃ³n completada exitosamente!")
    print("ğŸ’¡ Tu aplicaciÃ³n ahora usa el modelo mÃ¡s rÃ¡pido disponible.")

if __name__ == "__main__":
    main()